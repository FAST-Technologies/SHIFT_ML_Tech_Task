{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a02a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:94: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:263: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:94: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:263: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_977230/1746791803.py:94: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  train_df_processed['срок_займа_мес'] = train_df_processed['срок_займа'].str.extract('(\\d+)').astype(float)\n",
      "/tmp/ipykernel_977230/1746791803.py:263: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df_processed['срок_займа_мес'] = df_processed['срок_займа'].str.extract('(\\d+)').astype(float)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "\n",
    "class CreditScoringPreprocessor:\n",
    "    \"\"\"\n",
    "    Класс для предобработки данных с сохранением всех параметров\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, RANDOM_STATE: int = 42):\n",
    "        self.RANDOM_STATE = RANDOM_STATE\n",
    "        \n",
    "        # Фиксированные маппинги\n",
    "        self.rating_order = {'А': 1, 'Б': 2, 'В': 3, 'Г': 4, 'Д': 5, 'Е': 6, 'Ж': 7}\n",
    "        self.experience_order = {\n",
    "            '< 1 года': 0.5, '1 год': 1, '2 года': 2, '3 года': 3, \n",
    "            '4 года': 4, '5 лет': 5, '6 лет': 6, '7 лет': 7, \n",
    "            '8 лет': 8, '9 лет': 9, '10+ лет': 10\n",
    "        }\n",
    "        self.binary_mapping = {\n",
    "            'Да': 1, 'Нет': 0, 'Под вопросом': 0.5,\n",
    "            'True': 1, 'False': 0\n",
    "        }\n",
    "        \n",
    "        # Для сохранения параметров\n",
    "        self.medians_ = {}\n",
    "        self.freq_encoders_ = {}\n",
    "        self.scaler_ = None\n",
    "        self.onehot_categories_ = {}\n",
    "        self.feature_names_ = None\n",
    "        \n",
    "        # Признаки для удаления\n",
    "        self.useless_cols = [\n",
    "            'дата_следующей_выплаты',\n",
    "            'кредитный_баланс_по_возоб_счетам',\n",
    "            'совокупный_статус_подтверждения_доходов_заемщиков',\n",
    "            'совокупный_пдн_заемщиков',\n",
    "            'совокупный_доход_заемщиков',\n",
    "        ]\n",
    "        \n",
    "        self.constant_cols = [\n",
    "            'платежный_график',\n",
    "            'особая_ситуация',\n",
    "        ]\n",
    "        \n",
    "        self.binary_cols = ['пос_стоп_фактор', 'юридический_статус']\n",
    "        self.onehot_cols = [\n",
    "            'владение_жильем', 'подтвержден_ли_доход',\n",
    "            'первоначальный_статус_займа', 'тип_займа',\n",
    "            'тип_предоставления_кредита'\n",
    "        ]\n",
    "        self.freq_cols = ['профессия_заемщика', 'допрейтинг', 'регион']\n",
    "    \n",
    "    def fit(self, train_df: pd.DataFrame, target_col: str = 'итоговый_статус_займа'):\n",
    "        \"\"\"\n",
    "        Обучает препроцессор на train данных и сохраняет все параметры\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ОБУЧЕНИЕ ПРЕПРОЦЕССОРА\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        train_df_processed = train_df.copy()\n",
    "        \n",
    "        # Сохраняем ID и target\n",
    "        self.train_ids_ = train_df_processed['id'].copy()\n",
    "        if target_col in train_df_processed.columns:\n",
    "            self.y_ = train_df_processed[target_col].copy()\n",
    "        \n",
    "        print(f\"Начальный размер: {train_df_processed.shape}\")\n",
    "        \n",
    "        # 1. Удаление бесполезных признаков\n",
    "        cols_to_drop = [col for col in self.useless_cols + self.constant_cols \n",
    "                       if col in train_df_processed.columns]\n",
    "        train_df_processed = train_df_processed.drop(columns=cols_to_drop, errors='ignore')\n",
    "        print(f\"✓ Удалено {len(cols_to_drop)} признаков\")\n",
    "        \n",
    "        # 2. Обработка дат\n",
    "        if 'дата_первого_займа' in train_df_processed.columns:\n",
    "            train_df_processed['дата_первого_займа'] = pd.to_datetime(\n",
    "                train_df_processed['дата_первого_займа'], format='%m-%Y', errors='coerce'\n",
    "            )\n",
    "            current_date = pd.Timestamp('2026-01-01')\n",
    "            train_df_processed['стаж_кредитной_истории_мес'] = (\n",
    "                (current_date - train_df_processed['дата_первого_займа']).dt.days / 30\n",
    "            ).fillna(0)\n",
    "            train_df_processed['стаж_кредитной_истории_мес'] = train_df_processed['стаж_кредитной_истории_мес'].clip(0, 600)\n",
    "            train_df_processed = train_df_processed.drop(columns=['дата_первого_займа'])\n",
    "            print(\"✓ Создан признак: стаж_кредитной_истории_мес\")\n",
    "        \n",
    "        # 3. Обработка сроков займа\n",
    "        if 'срок_займа' in train_df_processed.columns:\n",
    "            train_df_processed['срок_займа_мес'] = train_df_processed['срок_займа'].str.extract('(\\d+)').astype(float)\n",
    "            train_df_processed['срок_займа_мес'] = train_df_processed['срок_займа_мес'] * 12\n",
    "            train_df_processed = train_df_processed.drop(columns=['срок_займа'])\n",
    "            print(\"✓ Создан признак: срок_займа_мес\")\n",
    "        \n",
    "        # 4. Кодирование рейтингов\n",
    "        if 'рейтинг' in train_df_processed.columns:\n",
    "            train_df_processed['рейтинг_encoded'] = train_df_processed['рейтинг'].map(self.rating_order).fillna(0)\n",
    "            train_df_processed = train_df_processed.drop(columns=['рейтинг'])\n",
    "            print(\"✓ Закодирован рейтинг\")\n",
    "        \n",
    "        # 5. Стаж работы\n",
    "        if 'стаж' in train_df_processed.columns:\n",
    "            train_df_processed['стаж_encoded'] = train_df_processed['стаж'].map(self.experience_order)\n",
    "            self.medians_['стаж_encoded'] = train_df_processed['стаж_encoded'].median()\n",
    "            train_df_processed['стаж_encoded'] = train_df_processed['стаж_encoded'].fillna(self.medians_['стаж_encoded'])\n",
    "            train_df_processed = train_df_processed.drop(columns=['стаж'])\n",
    "            print(\"✓ Закодирован стаж\")\n",
    "        \n",
    "        # 6. Бинарные признаки\n",
    "        for col in self.binary_cols:\n",
    "            if col in train_df_processed.columns:\n",
    "                train_df_processed[col] = train_df_processed[col].map(self.binary_mapping).fillna(0)\n",
    "                print(f\"✓ Закодирован {col}\")\n",
    "        \n",
    "        # 7. One-Hot Encoding (сохраняем категории)\n",
    "        for col in self.onehot_cols:\n",
    "            if col in train_df_processed.columns:\n",
    "                # Заполняем пропуски\n",
    "                train_df_processed[col] = train_df_processed[col].fillna('MISSING')\n",
    "                \n",
    "                # Сохраняем уникальные категории\n",
    "                self.onehot_categories_[col] = train_df_processed[col].unique().tolist()\n",
    "                print(f\"✓ Сохранены категории для {col}: {len(self.onehot_categories_[col])} значений\")\n",
    "        \n",
    "        # 8. Frequency Encoding (сохраняем частоты)\n",
    "        for col in self.freq_cols:\n",
    "            if col in train_df_processed.columns:\n",
    "                train_df_processed[col] = train_df_processed[col].fillna('MISSING')\n",
    "                freq = train_df_processed[col].value_counts(normalize=True)\n",
    "                self.freq_encoders_[col] = freq\n",
    "                print(f\"✓ Сохранены частоты для {col}\")\n",
    "        \n",
    "        # 9. Цель займа (группировка)\n",
    "        if 'цель_займа' in train_df_processed.columns:\n",
    "            purpose_groups = {\n",
    "                'консолидация_долга': ['консолидация_долга'],\n",
    "                'кредитная_карта': ['кредитная_карта'],\n",
    "                'жилье': ['улучшение_жилищных_условий', 'дом'],\n",
    "                'бизнес': ['мелкий_бизнес'],\n",
    "                'авто': ['автомобиль'],\n",
    "                'образование': ['образование'],\n",
    "                'лечение': ['лечение'],\n",
    "                'переезд': ['переезд'],\n",
    "                'отпуск': ['отпуск'],\n",
    "                'другое': ['другое', 'крупная_покупка', 'возобновляемая_энергия', 'свадьба']\n",
    "            }\n",
    "            \n",
    "            purpose_to_group = {}\n",
    "            for group, purposes in purpose_groups.items():\n",
    "                for purpose in purposes:\n",
    "                    purpose_to_group[purpose] = group\n",
    "            \n",
    "            train_df_processed['цель_займа'] = train_df_processed['цель_займа'].fillna('другое')\n",
    "            train_df_processed['цель_займа_группа'] = train_df_processed['цель_займа'].map(purpose_to_group)\n",
    "            train_df_processed.loc[train_df_processed['цель_займа_группа'].isna(), 'цель_займа_группа'] = 'другое'\n",
    "            \n",
    "            # Сохраняем уникальные группы\n",
    "            self.onehot_categories_['цель_займа_группа'] = train_df_processed['цель_займа_группа'].unique().tolist()\n",
    "            print(\"✓ Обработана цель_займа\")\n",
    "        \n",
    "        # 10. Обработка пени_за_дефолт\n",
    "        if 'пени_за_дефолт' in train_df_processed.columns:\n",
    "            train_df_processed['пени_за_дефолт'] = train_df_processed['пени_за_дефолт'].map({'True': 1, 'False': 0})\n",
    "            self.medians_['пени_за_дефолт'] = train_df_processed['пени_за_дефолт'].median()\n",
    "            print(\"✓ Обработано пени_за_дефолт\")\n",
    "        \n",
    "        # 11. Финансовые соотношения\n",
    "        if all(col in train_df_processed.columns for col in ['аннуитет', 'годовой_доход']):\n",
    "            train_df_processed['годовой_доход_safe'] = train_df_processed['годовой_доход'].replace(0, 1)\n",
    "            train_df_processed['аннуитет_к_доходу'] = train_df_processed['аннуитет'] * 12 / train_df_processed['годовой_доход_safe']\n",
    "            train_df_processed = train_df_processed.drop(columns=['годовой_доход_safe'])\n",
    "            print(\"✓ Создан: аннуитет_к_доходу\")\n",
    "        \n",
    "        if all(col in train_df_processed.columns for col in ['пдн', 'годовой_доход']):\n",
    "            train_df_processed['годовой_доход_safe'] = train_df_processed['годовой_доход'].replace(0, 1)\n",
    "            train_df_processed['пдн_от_дохода'] = train_df_processed['пдн'] / train_df_processed['годовой_доход_safe']\n",
    "            train_df_processed = train_df_processed.drop(columns=['годовой_доход_safe'])\n",
    "            print(\"✓ Создан: пдн_от_дохода\")\n",
    "        \n",
    "        if all(col in train_df_processed.columns for col in ['сумма_займа', 'годовой_доход']):\n",
    "            train_df_processed['годовой_доход_safe'] = train_df_processed['годовой_доход'].replace(0, 1)\n",
    "            train_df_processed['заем_к_доходу'] = train_df_processed['сумма_займа'] / train_df_processed['годовой_доход_safe']\n",
    "            train_df_processed = train_df_processed.drop(columns=['годовой_доход_safe'])\n",
    "            print(\"✓ Создан: заем_к_доходу\")\n",
    "        \n",
    "        # 12. Заполнение пропусков в числовых признаках\n",
    "        numeric_cols = train_df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        for col in numeric_cols:\n",
    "            if col in train_df_processed.columns and train_df_processed[col].isnull().any() and col != target_col:\n",
    "                self.medians_[col] = train_df_processed[col].median()\n",
    "                train_df_processed[col] = train_df_processed[col].fillna(self.medians_[col])\n",
    "        \n",
    "        # 13. Масштабирование\n",
    "        self.scaler_ = RobustScaler()\n",
    "        numeric_cols_to_scale = train_df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if numeric_cols_to_scale:\n",
    "            train_df_processed[numeric_cols_to_scale] = self.scaler_.fit_transform(train_df_processed[numeric_cols_to_scale])\n",
    "            print(f\"✓ Обучен scaler на {len(numeric_cols_to_scale)} признаках\")\n",
    "        \n",
    "        # 14. Удаление константных признаков\n",
    "        constant_cols_final = [col for col in train_df_processed.columns if train_df_processed[col].nunique() == 1]\n",
    "        if constant_cols_final:\n",
    "            train_df_processed = train_df_processed.drop(columns=constant_cols_final, errors='ignore')\n",
    "            print(f\"✓ Удалено {len(constant_cols_final)} константных признаков\")\n",
    "        \n",
    "        # Сохраняем имена фичей\n",
    "        self.feature_names_ = train_df_processed.columns.tolist()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ПРЕПРОЦЕССОР ОБУЧЕН!\")\n",
    "        print(f\"Количество признаков: {len(self.feature_names_)}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame, target_col: str = None):\n",
    "        \"\"\"\n",
    "        Применяет предобработку к новым данным с использованием сохраненных параметров\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ПРИМЕНЕНИЕ ПРЕОБРАБОТКИ К НОВЫМ ДАННЫМ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Сохраняем ID\n",
    "        if 'id' in df_processed.columns:\n",
    "            ids = df_processed['id'].copy()\n",
    "        else:\n",
    "            ids = None\n",
    "        \n",
    "        # Сохраняем target если есть\n",
    "        if target_col and target_col in df_processed.columns:\n",
    "            y = df_processed[target_col].copy()\n",
    "        else:\n",
    "            y = None\n",
    "        \n",
    "        print(f\"Начальный размер: {df_processed.shape}\")\n",
    "        \n",
    "        # 1. Удаление бесполезных признаков\n",
    "        cols_to_drop = [col for col in self.useless_cols + self.constant_cols \n",
    "                       if col in df_processed.columns]\n",
    "        df_processed = df_processed.drop(columns=cols_to_drop, errors='ignore')\n",
    "        \n",
    "        # 2. Обработка дат\n",
    "        if 'дата_первого_займа' in df_processed.columns:\n",
    "            df_processed['дата_первого_займа'] = pd.to_datetime(\n",
    "                df_processed['дата_первого_займа'], format='%m-%Y', errors='coerce'\n",
    "            )\n",
    "            current_date = pd.Timestamp('2026-01-01')\n",
    "            df_processed['стаж_кредитной_истории_мес'] = (\n",
    "                (current_date - df_processed['дата_первого_займа']).dt.days / 30\n",
    "            ).fillna(0)\n",
    "            df_processed['стаж_кредитной_истории_мес'] = df_processed['стаж_кредитной_истории_мес'].clip(0, 600)\n",
    "            df_processed = df_processed.drop(columns=['дата_первого_займа'])\n",
    "        \n",
    "        # 3. Обработка сроков займа\n",
    "        if 'срок_займа' in df_processed.columns:\n",
    "            df_processed['срок_займа_мес'] = df_processed['срок_займа'].str.extract('(\\d+)').astype(float)\n",
    "            df_processed['срок_займа_мес'] = df_processed['срок_займа_мес'] * 12\n",
    "            df_processed = df_processed.drop(columns=['срок_займа'])\n",
    "        \n",
    "        # 4. Кодирование рейтингов\n",
    "        if 'рейтинг' in df_processed.columns:\n",
    "            df_processed['рейтинг_encoded'] = df_processed['рейтинг'].map(self.rating_order).fillna(0)\n",
    "            df_processed = df_processed.drop(columns=['рейтинг'])\n",
    "        \n",
    "        # 5. Стаж работы\n",
    "        if 'стаж' in df_processed.columns:\n",
    "            df_processed['стаж_encoded'] = df_processed['стаж'].map(self.experience_order)\n",
    "            if 'стаж_encoded' in self.medians_:\n",
    "                df_processed['стаж_encoded'] = df_processed['стаж_encoded'].fillna(self.medians_['стаж_encoded'])\n",
    "            df_processed = df_processed.drop(columns=['стаж'])\n",
    "        \n",
    "        # 6. Бинарные признаки\n",
    "        for col in self.binary_cols:\n",
    "            if col in df_processed.columns:\n",
    "                df_processed[col] = df_processed[col].map(self.binary_mapping).fillna(0)\n",
    "        \n",
    "        # 7. One-Hot Encoding\n",
    "        for col in self.onehot_cols:\n",
    "            if col in df_processed.columns:\n",
    "                # Заполняем пропуски\n",
    "                df_processed[col] = df_processed[col].fillna('MISSING')\n",
    "                \n",
    "                # Создаем dummy-переменные только для сохраненных категорий\n",
    "                if col in self.onehot_categories_:\n",
    "                    for category in self.onehot_categories_[col]:\n",
    "                        df_processed[f'{col}_{category}'] = (df_processed[col] == category).astype(int)\n",
    "                \n",
    "                df_processed = df_processed.drop(columns=[col])\n",
    "        \n",
    "        # 8. Frequency Encoding\n",
    "        for col in self.freq_cols:\n",
    "            if col in df_processed.columns:\n",
    "                df_processed[col] = df_processed[col].fillna('MISSING')\n",
    "                if col in self.freq_encoders_:\n",
    "                    df_processed[f'{col}_freq_encoded'] = df_processed[col].map(self.freq_encoders_[col])\n",
    "                    # Заполняем пропуски средним значением частоты\n",
    "                    mean_freq = self.freq_encoders_[col].mean()\n",
    "                    df_processed[f'{col}_freq_encoded'] = df_processed[f'{col}_freq_encoded'].fillna(mean_freq)\n",
    "                df_processed = df_processed.drop(columns=[col])\n",
    "        \n",
    "        # 9. Цель займа (группировка)\n",
    "        if 'цель_займа' in df_processed.columns:\n",
    "            purpose_groups = {\n",
    "                'консолидация_долга': ['консолидация_долга'],\n",
    "                'кредитная_карта': ['кредитная_карта'],\n",
    "                'жилье': ['улучшение_жилищных_условий', 'дом'],\n",
    "                'бизнес': ['мелкий_бизнес'],\n",
    "                'авто': ['автомобиль'],\n",
    "                'образование': ['образование'],\n",
    "                'лечение': ['лечение'],\n",
    "                'переезд': ['переезд'],\n",
    "                'отпуск': ['отпуск'],\n",
    "                'другое': ['другое', 'крупная_покупка', 'возобновляемая_энергия', 'свадьба']\n",
    "            }\n",
    "            \n",
    "            purpose_to_group = {}\n",
    "            for group, purposes in purpose_groups.items():\n",
    "                for purpose in purposes:\n",
    "                    purpose_to_group[purpose] = group\n",
    "            \n",
    "            df_processed['цель_займа'] = df_processed['цель_займа'].fillna('другое')\n",
    "            df_processed['цель_займа_группа'] = df_processed['цель_займа'].map(purpose_to_group)\n",
    "            df_processed.loc[df_processed['цель_займа_группа'].isna(), 'цель_займа_группа'] = 'другое'\n",
    "            \n",
    "            # One-hot для сгруппированной цели\n",
    "            if 'цель_займа_группа' in self.onehot_categories_:\n",
    "                for category in self.onehot_categories_['цель_займа_группа']:\n",
    "                    df_processed[f'цель_займа_{category}'] = (df_processed['цель_займа_группа'] == category).astype(int)\n",
    "            \n",
    "            df_processed = df_processed.drop(columns=['цель_займа', 'цель_займа_группа'])\n",
    "        \n",
    "        # 10. Обработка пени_за_дефолт\n",
    "        if 'пени_за_дефолт' in df_processed.columns:\n",
    "            df_processed['пени_за_дефолт'] = df_processed['пени_за_дефолт'].map({'True': 1, 'False': 0})\n",
    "            if 'пени_за_дефолт' in self.medians_:\n",
    "                df_processed['пени_за_дефолт'] = df_processed['пени_за_дефолт'].fillna(self.medians_['пени_за_дефолт'])\n",
    "        \n",
    "        # 11. Финансовые соотношения\n",
    "        if all(col in df_processed.columns for col in ['аннуитет', 'годовой_доход']):\n",
    "            df_processed['годовой_доход_safe'] = df_processed['годовой_доход'].replace(0, 1)\n",
    "            df_processed['аннуитет_к_доходу'] = df_processed['аннуитет'] * 12 / df_processed['годовой_доход_safe']\n",
    "            df_processed = df_processed.drop(columns=['годовой_доход_safe'])\n",
    "        \n",
    "        if all(col in df_processed.columns for col in ['пдн', 'годовой_доход']):\n",
    "            df_processed['годовой_доход_safe'] = df_processed['годовой_доход'].replace(0, 1)\n",
    "            df_processed['пдн_от_дохода'] = df_processed['пдн'] / df_processed['годовой_доход_safe']\n",
    "            df_processed = df_processed.drop(columns=['годовой_доход_safe'])\n",
    "        \n",
    "        if all(col in df_processed.columns for col in ['сумма_займа', 'годовой_доход']):\n",
    "            df_processed['годовой_доход_safe'] = df_processed['годовой_доход'].replace(0, 1)\n",
    "            df_processed['заем_к_доходу'] = df_processed['сумма_займа'] / df_processed['годовой_доход_safe']\n",
    "            df_processed = df_processed.drop(columns=['годовой_доход_safe'])\n",
    "        \n",
    "        # 12. Заполнение пропусков в числовых признаках\n",
    "        for col, median_val in self.medians_.items():\n",
    "            if col in df_processed.columns:\n",
    "                df_processed[col] = df_processed[col].fillna(median_val)\n",
    "        \n",
    "        # 13. Масштабирование\n",
    "        if self.scaler_ is not None:\n",
    "            numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if numeric_cols:\n",
    "                df_processed[numeric_cols] = self.scaler_.transform(df_processed[numeric_cols])\n",
    "        \n",
    "        # 14. Выравнивание колонок с train\n",
    "        if self.feature_names_ is not None:\n",
    "            # Добавляем отсутствующие колонки\n",
    "            for col in self.feature_names_:\n",
    "                if col not in df_processed.columns:\n",
    "                    df_processed[col] = 0\n",
    "            \n",
    "            # Удаляем лишние колонки\n",
    "            extra_cols = [col for col in df_processed.columns if col not in self.feature_names_]\n",
    "            if extra_cols:\n",
    "                df_processed = df_processed.drop(columns=extra_cols)\n",
    "            \n",
    "            # Упорядочиваем колонки как в train\n",
    "            df_processed = df_processed[self.feature_names_]\n",
    "        \n",
    "        print(f\"Финальный размер: {df_processed.shape}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return df_processed, y, ids\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Сохраняет препроцессор на диск\"\"\"\n",
    "        joblib.dump(self, path)\n",
    "        print(f\"Препроцессор сохранен в {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\"Загружает препроцессор с диска\"\"\"\n",
    "        return joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1311932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_977230/2784789786.py:10: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('../data/shift_ml_2026_train.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ОБУЧЕНИЕ ПРЕПРОЦЕССОРА\n",
      "============================================================\n",
      "Начальный размер: (1210779, 109)\n",
      "✓ Удалено 7 признаков\n",
      "✓ Создан признак: стаж_кредитной_истории_мес\n",
      "✓ Создан признак: срок_займа_мес\n",
      "✓ Закодирован рейтинг\n",
      "✓ Закодирован стаж\n",
      "✓ Закодирован пос_стоп_фактор\n",
      "✓ Закодирован юридический_статус\n",
      "✓ Сохранены категории для владение_жильем: 6 значений\n",
      "✓ Сохранены категории для подтвержден_ли_доход: 3 значений\n",
      "✓ Сохранены категории для первоначальный_статус_займа: 2 значений\n",
      "✓ Сохранены категории для тип_займа: 2 значений\n",
      "✓ Сохранены категории для тип_предоставления_кредита: 2 значений\n",
      "✓ Сохранены частоты для профессия_заемщика\n",
      "✓ Сохранены частоты для допрейтинг\n",
      "✓ Сохранены частоты для регион\n",
      "✓ Обработана цель_займа\n",
      "✓ Обработано пени_за_дефолт\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamshchikov/ML_practice/Torchvision_core_project/MLvenv/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Создан: аннуитет_к_доходу\n",
      "✓ Создан: пдн_от_дохода\n",
      "✓ Создан: заем_к_доходу\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamshchikov/ML_practice/Torchvision_core_project/MLvenv/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/yamshchikov/ML_practice/Torchvision_core_project/MLvenv/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return fnb._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "/home/yamshchikov/ML_practice/Torchvision_core_project/MLvenv/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1409: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Обучен scaler на 96 признаках\n",
      "✓ Удалено 3 константных признаков\n",
      "\n",
      "============================================================\n",
      "ПРЕПРОЦЕССОР ОБУЧЕН!\n",
      "Количество признаков: 103\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ПРИМЕНЕНИЕ ПРЕОБРАБОТКИ К НОВЫМ ДАННЫМ\n",
      "============================================================\n",
      "Начальный размер: (1210779, 109)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_977230/1746791803.py:94: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  train_df_processed['срок_займа_мес'] = train_df_processed['срок_займа'].str.extract('(\\d+)').astype(float)\n",
      "/tmp/ipykernel_977230/1746791803.py:263: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df_processed['срок_займа_мес'] = df_processed['срок_займа'].str.extract('(\\d+)').astype(float)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- владение_жильем_АРЕНДА\n- владение_жильем_ДРУГОЕ\n- владение_жильем_ИПОТЕКА\n- владение_жильем_ЛЮБОЕ\n- владение_жильем_НЕТ\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m preprocessor.fit(train, target_col=\u001b[33m'\u001b[39m\u001b[33mитоговый_статус_займа\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Получаем обработанные train данные\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m X_train_all, y_train_all, train_ids_all = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mитоговый_статус_займа\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 3. Разделяем на train/val для валидации (80/20)\u001b[39;00m\n\u001b[32m     23\u001b[39m X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n\u001b[32m     24\u001b[39m     X_train_all, y_train_all, test_size=\u001b[32m0.2\u001b[39m, random_state=RANDOM_STATE, stratify=y_train_all\n\u001b[32m     25\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 370\u001b[39m, in \u001b[36mCreditScoringPreprocessor.transform\u001b[39m\u001b[34m(self, df, target_col)\u001b[39m\n\u001b[32m    368\u001b[39m     numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m numeric_cols:\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         df_processed[numeric_cols] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_processed\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# 14. Выравнивание колонок с train\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_names_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# Добавляем отсутствующие колонки\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_practice/Torchvision_core_project/MLvenv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_practice/Torchvision_core_project/MLvenv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1749\u001b[39m, in \u001b[36mRobustScaler.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1736\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Center and scale the data.\u001b[39;00m\n\u001b[32m   1737\u001b[39m \n\u001b[32m   1738\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1746\u001b[39m \u001b[33;03m    Transformed array.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1748\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1751\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1752\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1755\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1756\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1757\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1758\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1761\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_scaling:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_practice/Torchvision_core_project/MLvenv/lib/python3.12/site-packages/sklearn/utils/validation.py:2877\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2793\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2794\u001b[39m     _estimator,\n\u001b[32m   2795\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2801\u001b[39m     **check_params,\n\u001b[32m   2802\u001b[39m ):\n\u001b[32m   2803\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2804\u001b[39m \n\u001b[32m   2805\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2875\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2876\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2877\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2878\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2879\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_practice/Torchvision_core_project/MLvenv/lib/python3.12/site-packages/sklearn/utils/validation.py:2729\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2726\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2727\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2729\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- владение_жильем_АРЕНДА\n- владение_жильем_ДРУГОЕ\n- владение_жильем_ИПОТЕКА\n- владение_жильем_ЛЮБОЕ\n- владение_жильем_НЕТ\n- ...\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 1. Загружаем ИСХОДНЫЕ данные\n",
    "train = pd.read_csv('../data/shift_ml_2026_train.csv')\n",
    "test = pd.read_csv('../data/shift_ml_2026_test.csv')\n",
    "\n",
    "# 2. Создаем и обучаем препроцессор на ВСЕХ train данных\n",
    "preprocessor = CreditScoringPreprocessor(RANDOM_STATE=42)\n",
    "\n",
    "# Обучаем препроцессор на train данных\n",
    "preprocessor.fit(train, target_col='итоговый_статус_займа')\n",
    "\n",
    "# Получаем обработанные train данные\n",
    "X_train_all, y_train_all, train_ids_all = preprocessor.transform(train, target_col='итоговый_статус_займа')\n",
    "\n",
    "# 3. Разделяем на train/val для валидации (80/20)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_all, y_train_all, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train_all\n",
    ")\n",
    "\n",
    "print(f\"Размеры данных:\")\n",
    "print(f\"  X_train_split: {X_train_split.shape}\")\n",
    "print(f\"  X_val_split:   {X_val_split.shape}\")\n",
    "print(f\"  y_train_split: {y_train_split.shape}\")\n",
    "print(f\"  y_val_split:   {y_val_split.shape}\")\n",
    "\n",
    "# 4. Обучаем модель на train_split\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 31,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "lgb_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_val_split, y_val_split)],\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# 5. Оцениваем на val (для проверки)\n",
    "val_predictions = lgb_model.predict_proba(X_val_split)[:, 1]\n",
    "val_auc = roc_auc_score(y_val_split, val_predictions)\n",
    "print(f\"\\nVal AUC: {val_auc:.4f}\")\n",
    "\n",
    "# 6. Переобучаем на ВСЕХ train данных для финального сабмита\n",
    "final_model = lgb.LGBMClassifier(**lgb_params)\n",
    "final_model.fit(X_train_all, y_train_all)\n",
    "\n",
    "print(f\"\\nФинальная модель обучена на {len(X_train_all)} записях\")\n",
    "\n",
    "# 7. Применяем препроцессор к ИСХОДНОМУ test\n",
    "X_test_processed, _, test_ids = preprocessor.transform(test)\n",
    "\n",
    "print(f\"\\nTest данные обработаны:\")\n",
    "print(f\"  X_test_processed: {X_test_processed.shape}\")\n",
    "print(f\"  test_ids: {len(test_ids)}\")\n",
    "\n",
    "# Проверяем, что признаки совпадают\n",
    "print(f\"\\nПроверка совпадения признаков:\")\n",
    "print(f\"  Train features: {len(X_train_all.columns)}\")\n",
    "print(f\"  Test features:  {len(X_test_processed.columns)}\")\n",
    "\n",
    "if list(X_train_all.columns) == list(X_test_processed.columns):\n",
    "    print(\"✓ Признаки совпадают!\")\n",
    "else:\n",
    "    print(\"⚠ Признаки не совпадают!\")\n",
    "    # Находим различия\n",
    "    train_cols = set(X_train_all.columns)\n",
    "    test_cols = set(X_test_processed.columns)\n",
    "    print(f\"  В train, но нет в test: {train_cols - test_cols}\")\n",
    "    print(f\"  В test, но нет в train: {test_cols - train_cols}\")\n",
    "\n",
    "# 8. Предсказания на тесте\n",
    "test_predictions = final_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(f\"\\nСтатистика предсказаний на тесте:\")\n",
    "print(f\"  Min:  {test_predictions.min():.6f}\")\n",
    "print(f\"  Max:  {test_predictions.max():.6f}\")\n",
    "print(f\"  Mean: {test_predictions.mean():.6f}\")\n",
    "print(f\"  Std:  {test_predictions.std():.6f}\")\n",
    "\n",
    "# Для сравнения - предсказания на train\n",
    "train_predictions = final_model.predict_proba(X_train_all)[:, 1]\n",
    "print(f\"\\nСтатистика предсказаний на train:\")\n",
    "print(f\"  Min:  {train_predictions.min():.6f}\")\n",
    "print(f\"  Max:  {train_predictions.max():.6f}\")\n",
    "print(f\"  Mean: {train_predictions.mean():.6f}\")\n",
    "print(f\"  Std:  {train_predictions.std():.6f}\")\n",
    "\n",
    "# 9. Создаем сабмит\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'Proba': test_predictions\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ПРОВЕРКА САБМИТА:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Размер: {submission.shape}\")\n",
    "print(f\"Колонки: {submission.columns.tolist()}\")\n",
    "print(f\"Типы данных:\\n{submission.dtypes}\")\n",
    "\n",
    "print(\"\\nПервые 10 строк сабмита:\")\n",
    "print(submission.head(10).to_string(index=False))\n",
    "\n",
    "# Проверка на NaN\n",
    "if submission['Proba'].isna().any():\n",
    "    nan_count = submission['Proba'].isna().sum()\n",
    "    print(f\"\\n⚠ ВНИМАНИЕ: Найдено {nan_count} NaN значений!\")\n",
    "    # Заполняем средним\n",
    "    mean_val = submission['Proba'].mean()\n",
    "    submission['Proba'] = submission['Proba'].fillna(mean_val)\n",
    "    print(f\"Заполнено средним значением: {mean_val:.6f}\")\n",
    "\n",
    "# Сохраняем сабмит\n",
    "submission.to_csv('final_submission_improved.csv', index=False)\n",
    "print(f\"\\n✅ Сабмит сохранен: final_submission_improved.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Кросс-валидация\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    lgb_model, X_train_all, y_train_all,\n",
    "    cv=cv, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
